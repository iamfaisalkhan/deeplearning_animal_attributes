{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL3\n",
    "Follow this notebook only if you're new to DeepLearing and Transfer learning. This is a extension of the Startet kit given [here](https://github.com/shubham3121/DL-3/blob/master/DL%233_EDA.ipynb). I'll try to keep it simple. Please ignore the typos :)\n",
    "\n",
    "### Why to use Mobilenet architecture?\n",
    "You might have seen multiple tutorials on the VGG16 based transfer learning but here I'm going to use Mobilenet because of the following reasons \n",
    "<ul>\n",
    "    <li> No. of parameters to train in Mobilenet is quite less in compare to the VGG16\n",
    "    <li> Having fewer parameters will make your training time less and you'll be able to do more experiment and your chances of wining becames higher.\n",
    "    <li> On top of above reasons Mobile net has similar performance on the ImageNet dataset as VGG16\n",
    "</ul>\n",
    "\n",
    "Having said that let move on to the imorting important libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/faisal/anaconda3/envs/ai/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.applications import MobileNet\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "# The below is necessary in Python 3.2.3 onwards to\n",
    "# have reproducible behavior for certain hash-based operations.\n",
    "# See these references for further details:\n",
    "# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED\n",
    "# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "\n",
    "rn.seed(12345)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of\n",
    "# non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to use 128x128 images. You can change that if you wish.\n",
    "\n",
    "My folder structure is as follow\n",
    "\n",
    "<ul>\n",
    "    <li> DL3</li>\n",
    "        <ul>\n",
    "            <li> starter_kit</li>\n",
    "                <ul>\n",
    "                    <li> this_notebook</li>\n",
    "                </ul>\n",
    "            <li> data</li>\n",
    "                <ul>\n",
    "                    <li> train_img</li>\n",
    "                    <li> test_img</li>\n",
    "                </ul>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = (224, 224)\n",
    "\n",
    "train_data_dir = './data/train_img/'\n",
    "test_data_dir = './data/test_img/'\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mobile_model = MobileNet(include_top=False, input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "# add a global spatial average pooling layer\n",
    "    x = Mobile_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    predictions = Dense(85, activation='sigmoid')(x)\n",
    "    model = Model(inputs=Mobile_model.input, outputs=predictions)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with training the head(last layer) only as that layer is initialized randomaly and we don't want to affect the other layers weights as while backpropogation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (Activation)      (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (Activation)  (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (Activation)  (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (Activation)  (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (Activation)  (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (Activation)  (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (Activation) (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 85)                87125     \n",
      "=================================================================\n",
      "Total params: 3,315,989\n",
      "Trainable params: 87,125\n",
      "Non-trainable params: 3,228,864\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#train only last layer\n",
    "for layer in model.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(width_shift_range=0.2, \n",
    "                                   height_shift_range=0.2,\n",
    "                                   rescale=1. / 255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True, \n",
    "                                   rotation_range = 20,\n",
    "#                                    zca_whitening=True\n",
    "                                   )\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv', index_col=0)\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "attributes = pd.read_csv('./data/attributes.txt', delimiter='\\t', header=None, index_col=0)\n",
    "classes = pd.read_csv('./data/classes.txt', delimiter='\\t', header=None, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imgs(src, df, labels = False):\n",
    "    if labels == False:\n",
    "        imgs = []    \n",
    "        files = df['Image_name'].values\n",
    "        for file in tqdm(files):\n",
    "            im = cv.imread(os.path.join(src, file))\n",
    "            im = cv.resize(im, (img_width, img_height))\n",
    "            imgs.append(im)\n",
    "        return np.array(imgs)\n",
    "    else:\n",
    "        imgs = []\n",
    "        labels = []\n",
    "        files = os.listdir(src)\n",
    "        for file in tqdm(files):\n",
    "            im = cv.imread(os.path.join(src, file))\n",
    "            im = cv.resize(im, (img_width, img_height))\n",
    "            imgs.append(im)\n",
    "            labels.append(df.loc[file].values)\n",
    "        return np.array(imgs), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 313/12600 [00:09<06:28, 31.64it/s] /home/faisal/anaconda3/envs/ai/lib/python3.6/site-packages/tqdm/_monitor.py:89: TqdmSynchronisationWarning: Set changed size during iteration (see https://github.com/tqdm/tqdm/issues/481)\n",
      "  TqdmSynchronisationWarning)\n",
      "100%|██████████| 12600/12600 [08:46<00:00, 23.92it/s]\n"
     ]
    }
   ],
   "source": [
    "train_imgs, train_labels = get_imgs(train_data_dir, train, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49989"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train val split\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(train_imgs, train_labels, test_size = 3000, random_state = 222)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9600, 224, 224, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen.fit(X_tra)\n",
    "val_datagen.fit(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmeasure(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to train our model with SGD and very low learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stp = EarlyStopping(monitor=\"val_loss\", mode='min', patience=5)\n",
    "model_ckpt = ModelCheckpoint('mobilenet_1_layer.h5', save_best_only=True, mode='min', monitor='val_loss', verbose=1, save_weights_only=True)\n",
    "\n",
    "# opt = optimizers.SGD(lr=0.001, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "opt = optimizers.Adam(lr=1e-4)\n",
    "model.compile(opt, loss = 'binary_crossentropy', metrics=['accuracy', fmeasure])\n",
    "model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "75/75 [==============================] - 22s 288ms/step - loss: 0.6029 - acc: 0.6839 - fmeasure: 0.5613 - val_loss: 0.5282 - val_acc: 0.7435 - val_fmeasure: 0.6278\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.52820, saving model to mobilenet_1_layer.h5\n",
      "Epoch 2/100\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 0.4968 - acc: 0.7618 - fmeasure: 0.6562 - val_loss: 0.4746 - val_acc: 0.7745 - val_fmeasure: 0.6736\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.52820 to 0.47458, saving model to mobilenet_1_layer.h5\n",
      "Epoch 3/100\n",
      "75/75 [==============================] - 18s 243ms/step - loss: 0.4564 - acc: 0.7862 - fmeasure: 0.6925 - val_loss: 0.4376 - val_acc: 0.7958 - val_fmeasure: 0.7057\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.47458 to 0.43761, saving model to mobilenet_1_layer.h5\n",
      "Epoch 4/100\n",
      "75/75 [==============================] - 18s 245ms/step - loss: 0.4248 - acc: 0.8050 - fmeasure: 0.7207 - val_loss: 0.4084 - val_acc: 0.8118 - val_fmeasure: 0.7311\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.43761 to 0.40839, saving model to mobilenet_1_layer.h5\n",
      "Epoch 5/100\n",
      "75/75 [==============================] - 18s 246ms/step - loss: 0.4000 - acc: 0.8196 - fmeasure: 0.7423 - val_loss: 0.3855 - val_acc: 0.8249 - val_fmeasure: 0.7515\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.40839 to 0.38550, saving model to mobilenet_1_layer.h5\n",
      "Epoch 6/100\n",
      "75/75 [==============================] - 18s 244ms/step - loss: 0.3806 - acc: 0.8299 - fmeasure: 0.7580 - val_loss: 0.3688 - val_acc: 0.8340 - val_fmeasure: 0.7639\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.38550 to 0.36878, saving model to mobilenet_1_layer.h5\n",
      "Epoch 7/100\n",
      "75/75 [==============================] - 19s 248ms/step - loss: 0.3636 - acc: 0.8390 - fmeasure: 0.7714 - val_loss: 0.3547 - val_acc: 0.8411 - val_fmeasure: 0.7754\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.36878 to 0.35468, saving model to mobilenet_1_layer.h5\n",
      "Epoch 8/100\n",
      "75/75 [==============================] - 19s 248ms/step - loss: 0.3508 - acc: 0.8457 - fmeasure: 0.7816 - val_loss: 0.3430 - val_acc: 0.8479 - val_fmeasure: 0.7855\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35468 to 0.34303, saving model to mobilenet_1_layer.h5\n",
      "Epoch 9/100\n",
      "75/75 [==============================] - 18s 246ms/step - loss: 0.3392 - acc: 0.8519 - fmeasure: 0.7907 - val_loss: 0.3324 - val_acc: 0.8531 - val_fmeasure: 0.7933\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.34303 to 0.33243, saving model to mobilenet_1_layer.h5\n",
      "Epoch 10/100\n",
      "75/75 [==============================] - 19s 248ms/step - loss: 0.3288 - acc: 0.8577 - fmeasure: 0.7992 - val_loss: 0.3279 - val_acc: 0.8550 - val_fmeasure: 0.7962\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.33243 to 0.32785, saving model to mobilenet_1_layer.h5\n",
      "Epoch 11/100\n",
      "75/75 [==============================] - 18s 245ms/step - loss: 0.3204 - acc: 0.8620 - fmeasure: 0.8054 - val_loss: 0.3162 - val_acc: 0.8617 - val_fmeasure: 0.8067\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.32785 to 0.31619, saving model to mobilenet_1_layer.h5\n",
      "Epoch 12/100\n",
      "75/75 [==============================] - 18s 242ms/step - loss: 0.3132 - acc: 0.8654 - fmeasure: 0.8107 - val_loss: 0.3125 - val_acc: 0.8633 - val_fmeasure: 0.8091\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.31619 to 0.31253, saving model to mobilenet_1_layer.h5\n",
      "Epoch 13/100\n",
      "75/75 [==============================] - 18s 245ms/step - loss: 0.3072 - acc: 0.8686 - fmeasure: 0.8152 - val_loss: 0.3074 - val_acc: 0.8659 - val_fmeasure: 0.8135\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.31253 to 0.30735, saving model to mobilenet_1_layer.h5\n",
      "Epoch 14/100\n",
      "75/75 [==============================] - 18s 245ms/step - loss: 0.3017 - acc: 0.8711 - fmeasure: 0.8191 - val_loss: 0.3070 - val_acc: 0.8659 - val_fmeasure: 0.8128\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.30735 to 0.30696, saving model to mobilenet_1_layer.h5\n",
      "Epoch 15/100\n",
      "75/75 [==============================] - 18s 244ms/step - loss: 0.2962 - acc: 0.8740 - fmeasure: 0.8232 - val_loss: 0.2978 - val_acc: 0.8711 - val_fmeasure: 0.8213\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.30696 to 0.29776, saving model to mobilenet_1_layer.h5\n",
      "Epoch 16/100\n",
      "75/75 [==============================] - 19s 250ms/step - loss: 0.2913 - acc: 0.8760 - fmeasure: 0.8262 - val_loss: 0.2936 - val_acc: 0.8732 - val_fmeasure: 0.8236\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.29776 to 0.29364, saving model to mobilenet_1_layer.h5\n",
      "Epoch 17/100\n",
      "75/75 [==============================] - 18s 246ms/step - loss: 0.2865 - acc: 0.8787 - fmeasure: 0.8301 - val_loss: 0.2905 - val_acc: 0.8749 - val_fmeasure: 0.8265\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.29364 to 0.29052, saving model to mobilenet_1_layer.h5\n",
      "Epoch 18/100\n",
      "75/75 [==============================] - 19s 247ms/step - loss: 0.2839 - acc: 0.8798 - fmeasure: 0.8318 - val_loss: 0.2897 - val_acc: 0.8751 - val_fmeasure: 0.8264\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.29052 to 0.28973, saving model to mobilenet_1_layer.h5\n",
      "Epoch 19/100\n",
      "75/75 [==============================] - 18s 245ms/step - loss: 0.2795 - acc: 0.8823 - fmeasure: 0.8353 - val_loss: 0.2850 - val_acc: 0.8775 - val_fmeasure: 0.8304\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.28973 to 0.28496, saving model to mobilenet_1_layer.h5\n",
      "Epoch 20/100\n",
      "75/75 [==============================] - 18s 245ms/step - loss: 0.2770 - acc: 0.8833 - fmeasure: 0.8368 - val_loss: 0.2836 - val_acc: 0.8780 - val_fmeasure: 0.8316\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.28496 to 0.28355, saving model to mobilenet_1_layer.h5\n",
      "Epoch 21/100\n",
      "75/75 [==============================] - 18s 245ms/step - loss: 0.2725 - acc: 0.8852 - fmeasure: 0.8396 - val_loss: 0.2806 - val_acc: 0.8794 - val_fmeasure: 0.8333\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.28355 to 0.28057, saving model to mobilenet_1_layer.h5\n",
      "Epoch 22/100\n",
      "75/75 [==============================] - 18s 244ms/step - loss: 0.2699 - acc: 0.8868 - fmeasure: 0.8420 - val_loss: 0.2809 - val_acc: 0.8791 - val_fmeasure: 0.8330\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/100\n",
      "75/75 [==============================] - 18s 246ms/step - loss: 0.2674 - acc: 0.8876 - fmeasure: 0.8433 - val_loss: 0.2776 - val_acc: 0.8808 - val_fmeasure: 0.8353\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.28057 to 0.27763, saving model to mobilenet_1_layer.h5\n",
      "Epoch 24/100\n",
      "75/75 [==============================] - 18s 245ms/step - loss: 0.2651 - acc: 0.8886 - fmeasure: 0.8446 - val_loss: 0.2756 - val_acc: 0.8820 - val_fmeasure: 0.8369\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.27763 to 0.27560, saving model to mobilenet_1_layer.h5\n",
      "Epoch 25/100\n",
      "75/75 [==============================] - 18s 245ms/step - loss: 0.2623 - acc: 0.8899 - fmeasure: 0.8465 - val_loss: 0.2716 - val_acc: 0.8839 - val_fmeasure: 0.8399\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.27560 to 0.27155, saving model to mobilenet_1_layer.h5\n",
      "Epoch 26/100\n",
      "75/75 [==============================] - 18s 245ms/step - loss: 0.2612 - acc: 0.8907 - fmeasure: 0.8477 - val_loss: 0.2700 - val_acc: 0.8846 - val_fmeasure: 0.8410\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.27155 to 0.26998, saving model to mobilenet_1_layer.h5\n",
      "Epoch 27/100\n",
      "75/75 [==============================] - 18s 244ms/step - loss: 0.2584 - acc: 0.8915 - fmeasure: 0.8487 - val_loss: 0.2666 - val_acc: 0.8864 - val_fmeasure: 0.8437\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.26998 to 0.26665, saving model to mobilenet_1_layer.h5\n",
      "Epoch 28/100\n",
      "75/75 [==============================] - 18s 243ms/step - loss: 0.2571 - acc: 0.8923 - fmeasure: 0.8498 - val_loss: 0.2685 - val_acc: 0.8851 - val_fmeasure: 0.8423\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/100\n",
      "75/75 [==============================] - 18s 244ms/step - loss: 0.2552 - acc: 0.8935 - fmeasure: 0.8518 - val_loss: 0.2663 - val_acc: 0.8865 - val_fmeasure: 0.8436\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.26665 to 0.26634, saving model to mobilenet_1_layer.h5\n",
      "Epoch 30/100\n",
      "75/75 [==============================] - 18s 245ms/step - loss: 0.2525 - acc: 0.8944 - fmeasure: 0.8529 - val_loss: 0.2661 - val_acc: 0.8865 - val_fmeasure: 0.8437\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.26634 to 0.26610, saving model to mobilenet_1_layer.h5\n",
      "Epoch 31/100\n",
      "75/75 [==============================] - 19s 247ms/step - loss: 0.2518 - acc: 0.8948 - fmeasure: 0.8535 - val_loss: 0.2625 - val_acc: 0.8881 - val_fmeasure: 0.8462\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.26610 to 0.26254, saving model to mobilenet_1_layer.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n",
      "75/75 [==============================] - 18s 244ms/step - loss: 0.2502 - acc: 0.8954 - fmeasure: 0.8545 - val_loss: 0.2652 - val_acc: 0.8870 - val_fmeasure: 0.8445\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/100\n",
      "75/75 [==============================] - 19s 247ms/step - loss: 0.2492 - acc: 0.8956 - fmeasure: 0.8547 - val_loss: 0.2615 - val_acc: 0.8888 - val_fmeasure: 0.8475\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.26254 to 0.26149, saving model to mobilenet_1_layer.h5\n",
      "Epoch 34/100\n",
      "75/75 [==============================] - 19s 247ms/step - loss: 0.2460 - acc: 0.8977 - fmeasure: 0.8578 - val_loss: 0.2610 - val_acc: 0.8886 - val_fmeasure: 0.8468\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.26149 to 0.26098, saving model to mobilenet_1_layer.h5\n",
      "Epoch 35/100\n",
      "75/75 [==============================] - 18s 246ms/step - loss: 0.2465 - acc: 0.8969 - fmeasure: 0.8565 - val_loss: 0.2601 - val_acc: 0.8892 - val_fmeasure: 0.8481\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.26098 to 0.26009, saving model to mobilenet_1_layer.h5\n",
      "Epoch 36/100\n",
      "75/75 [==============================] - 18s 244ms/step - loss: 0.2433 - acc: 0.8988 - fmeasure: 0.8593 - val_loss: 0.2591 - val_acc: 0.8897 - val_fmeasure: 0.8487\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.26009 to 0.25906, saving model to mobilenet_1_layer.h5\n",
      "Epoch 37/100\n",
      "75/75 [==============================] - 18s 243ms/step - loss: 0.2424 - acc: 0.8993 - fmeasure: 0.8601 - val_loss: 0.2584 - val_acc: 0.8900 - val_fmeasure: 0.8490\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.25906 to 0.25837, saving model to mobilenet_1_layer.h5\n",
      "Epoch 38/100\n",
      "75/75 [==============================] - 18s 244ms/step - loss: 0.2416 - acc: 0.8994 - fmeasure: 0.8602 - val_loss: 0.2581 - val_acc: 0.8902 - val_fmeasure: 0.8491\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.25837 to 0.25812, saving model to mobilenet_1_layer.h5\n",
      "Epoch 39/100\n",
      "75/75 [==============================] - 19s 249ms/step - loss: 0.2414 - acc: 0.8989 - fmeasure: 0.8595 - val_loss: 0.2581 - val_acc: 0.8902 - val_fmeasure: 0.8492\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/100\n",
      "75/75 [==============================] - 18s 245ms/step - loss: 0.2400 - acc: 0.9001 - fmeasure: 0.8612 - val_loss: 0.2580 - val_acc: 0.8905 - val_fmeasure: 0.8496\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.25812 to 0.25796, saving model to mobilenet_1_layer.h5\n",
      "Epoch 41/100\n",
      "75/75 [==============================] - 18s 245ms/step - loss: 0.2379 - acc: 0.9010 - fmeasure: 0.8625 - val_loss: 0.2536 - val_acc: 0.8926 - val_fmeasure: 0.8524\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.25796 to 0.25359, saving model to mobilenet_1_layer.h5\n",
      "Epoch 42/100\n",
      "75/75 [==============================] - 18s 244ms/step - loss: 0.2382 - acc: 0.9004 - fmeasure: 0.8616 - val_loss: 0.2538 - val_acc: 0.8924 - val_fmeasure: 0.8523\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/100\n",
      "75/75 [==============================] - 19s 250ms/step - loss: 0.2376 - acc: 0.9011 - fmeasure: 0.8627 - val_loss: 0.2540 - val_acc: 0.8920 - val_fmeasure: 0.8516\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/100\n",
      "75/75 [==============================] - 18s 246ms/step - loss: 0.2361 - acc: 0.9016 - fmeasure: 0.8635 - val_loss: 0.2551 - val_acc: 0.8916 - val_fmeasure: 0.8511\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/100\n",
      "75/75 [==============================] - 19s 247ms/step - loss: 0.2351 - acc: 0.9021 - fmeasure: 0.8640 - val_loss: 0.2509 - val_acc: 0.8939 - val_fmeasure: 0.8544\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.25359 to 0.25090, saving model to mobilenet_1_layer.h5\n",
      "Epoch 46/100\n",
      "75/75 [==============================] - 18s 246ms/step - loss: 0.2331 - acc: 0.9032 - fmeasure: 0.8657 - val_loss: 0.2517 - val_acc: 0.8932 - val_fmeasure: 0.8535\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/100\n",
      "75/75 [==============================] - 18s 245ms/step - loss: 0.2334 - acc: 0.9032 - fmeasure: 0.8656 - val_loss: 0.2509 - val_acc: 0.8938 - val_fmeasure: 0.8545\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.25090 to 0.25089, saving model to mobilenet_1_layer.h5\n",
      "Epoch 48/100\n",
      "75/75 [==============================] - 18s 245ms/step - loss: 0.2317 - acc: 0.9035 - fmeasure: 0.8661 - val_loss: 0.2492 - val_acc: 0.8947 - val_fmeasure: 0.8553\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.25089 to 0.24918, saving model to mobilenet_1_layer.h5\n",
      "Epoch 49/100\n",
      "75/75 [==============================] - 18s 246ms/step - loss: 0.2322 - acc: 0.9034 - fmeasure: 0.8659 - val_loss: 0.2497 - val_acc: 0.8942 - val_fmeasure: 0.8547\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/100\n",
      "75/75 [==============================] - 19s 248ms/step - loss: 0.2307 - acc: 0.9042 - fmeasure: 0.8672 - val_loss: 0.2494 - val_acc: 0.8945 - val_fmeasure: 0.8549\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/100\n",
      "75/75 [==============================] - 19s 249ms/step - loss: 0.2307 - acc: 0.9042 - fmeasure: 0.8669 - val_loss: 0.2517 - val_acc: 0.8934 - val_fmeasure: 0.8538\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/100\n",
      "75/75 [==============================] - 19s 249ms/step - loss: 0.2293 - acc: 0.9045 - fmeasure: 0.8676 - val_loss: 0.2495 - val_acc: 0.8946 - val_fmeasure: 0.8554\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/100\n",
      "75/75 [==============================] - 19s 247ms/step - loss: 0.2274 - acc: 0.9056 - fmeasure: 0.8691 - val_loss: 0.2465 - val_acc: 0.8958 - val_fmeasure: 0.8572\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.24918 to 0.24652, saving model to mobilenet_1_layer.h5\n",
      "Epoch 54/100\n",
      "75/75 [==============================] - 18s 245ms/step - loss: 0.2288 - acc: 0.9047 - fmeasure: 0.8678 - val_loss: 0.2477 - val_acc: 0.8955 - val_fmeasure: 0.8566\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/100\n",
      "75/75 [==============================] - 19s 247ms/step - loss: 0.2255 - acc: 0.9067 - fmeasure: 0.8706 - val_loss: 0.2445 - val_acc: 0.8969 - val_fmeasure: 0.8587\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.24652 to 0.24449, saving model to mobilenet_1_layer.h5\n",
      "Epoch 56/100\n",
      "75/75 [==============================] - 18s 245ms/step - loss: 0.2259 - acc: 0.9063 - fmeasure: 0.8700 - val_loss: 0.2461 - val_acc: 0.8962 - val_fmeasure: 0.8575\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/100\n",
      "75/75 [==============================] - 18s 245ms/step - loss: 0.2260 - acc: 0.9062 - fmeasure: 0.8699 - val_loss: 0.2453 - val_acc: 0.8966 - val_fmeasure: 0.8580\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/100\n",
      "75/75 [==============================] - 19s 250ms/step - loss: 0.2251 - acc: 0.9063 - fmeasure: 0.8701 - val_loss: 0.2454 - val_acc: 0.8966 - val_fmeasure: 0.8582\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/100\n",
      "75/75 [==============================] - 18s 246ms/step - loss: 0.2239 - acc: 0.9073 - fmeasure: 0.8714 - val_loss: 0.2446 - val_acc: 0.8968 - val_fmeasure: 0.8588\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/100\n",
      "75/75 [==============================] - 18s 245ms/step - loss: 0.2241 - acc: 0.9069 - fmeasure: 0.8710 - val_loss: 0.2428 - val_acc: 0.8978 - val_fmeasure: 0.8598\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.24449 to 0.24279, saving model to mobilenet_1_layer.h5\n",
      "Epoch 61/100\n",
      "75/75 [==============================] - 19s 248ms/step - loss: 0.2239 - acc: 0.9069 - fmeasure: 0.8709 - val_loss: 0.2411 - val_acc: 0.8987 - val_fmeasure: 0.8609\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.24279 to 0.24112, saving model to mobilenet_1_layer.h5\n",
      "Epoch 62/100\n",
      "75/75 [==============================] - 19s 248ms/step - loss: 0.2220 - acc: 0.9082 - fmeasure: 0.8726 - val_loss: 0.2421 - val_acc: 0.8980 - val_fmeasure: 0.8602\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 63/100\n",
      "75/75 [==============================] - 19s 252ms/step - loss: 0.2231 - acc: 0.9075 - fmeasure: 0.8718 - val_loss: 0.2441 - val_acc: 0.8972 - val_fmeasure: 0.8595\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 64/100\n",
      "75/75 [==============================] - 19s 249ms/step - loss: 0.2211 - acc: 0.9079 - fmeasure: 0.8724 - val_loss: 0.2444 - val_acc: 0.8970 - val_fmeasure: 0.8587\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 65/100\n",
      "75/75 [==============================] - 19s 247ms/step - loss: 0.2205 - acc: 0.9085 - fmeasure: 0.8732 - val_loss: 0.2434 - val_acc: 0.8977 - val_fmeasure: 0.8595\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 19s 248ms/step - loss: 0.2206 - acc: 0.9086 - fmeasure: 0.8734 - val_loss: 0.2422 - val_acc: 0.8982 - val_fmeasure: 0.8603\n",
      "\n",
      "Epoch 00066: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3c775ac668>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_datagen.flow(X_tra, y_tra, batch_size=batch_size),                     \n",
    "                    steps_per_epoch=len(X_tra) / batch_size, epochs=10,\n",
    "                    validation_data=val_datagen.flow(X_val, y_val, batch_size=batch_size), \n",
    "                    validation_steps = len(X_val)/batch_size, callbacks=[early_stp, model_ckpt], workers = 10, max_queue_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (Activation)      (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (Activation)  (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (Activation)  (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (Activation)  (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (Activation)  (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (Activation)  (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (Activation) (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 85)                87125     \n",
      "=================================================================\n",
      "Total params: 3,315,989\n",
      "Trainable params: 3,294,101\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "\n",
    "#train only last 10 layer\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "opt = optimizers.Adam(lr=1e-4)\n",
    "# opt = optimizers.SGD(lr=0.001, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "model.compile(opt, loss = 'binary_crossentropy', metrics=['accuracy', fmeasure])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stp = EarlyStopping(monitor=\"val_loss\", mode='min', patience=5)\n",
    "model_ckpt = ModelCheckpoint('mobilenet_all_layers.h5', save_best_only=True, mode='min', monitor='val_loss', verbose=1, save_weights_only=True)\n",
    "\n",
    "model.load_weights('mobilenet_1_layer.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "150/150 [==============================] - 52s 347ms/step - loss: 0.2003 - acc: 0.9170 - fmeasure: 0.8852 - val_loss: 0.1775 - val_acc: 0.9282 - val_fmeasure: 0.9006\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.17746, saving model to mobilenet_all_layers.h5\n",
      "Epoch 2/100\n",
      "150/150 [==============================] - 50s 333ms/step - loss: 0.1589 - acc: 0.9360 - fmeasure: 0.9117 - val_loss: 0.1694 - val_acc: 0.9316 - val_fmeasure: 0.9048\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.17746 to 0.16941, saving model to mobilenet_all_layers.h5\n",
      "Epoch 3/100\n",
      "150/150 [==============================] - 51s 340ms/step - loss: 0.1372 - acc: 0.9455 - fmeasure: 0.9249 - val_loss: 0.1556 - val_acc: 0.9379 - val_fmeasure: 0.9148\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.16941 to 0.15560, saving model to mobilenet_all_layers.h5\n",
      "Epoch 4/100\n",
      "150/150 [==============================] - 52s 344ms/step - loss: 0.1194 - acc: 0.9534 - fmeasure: 0.9358 - val_loss: 0.1465 - val_acc: 0.9418 - val_fmeasure: 0.9191\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.15560 to 0.14650, saving model to mobilenet_all_layers.h5\n",
      "Epoch 5/100\n",
      "150/150 [==============================] - 52s 344ms/step - loss: 0.1057 - acc: 0.9596 - fmeasure: 0.9445 - val_loss: 0.1255 - val_acc: 0.9500 - val_fmeasure: 0.9313\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.14650 to 0.12550, saving model to mobilenet_all_layers.h5\n",
      "Epoch 6/100\n",
      "150/150 [==============================] - 51s 341ms/step - loss: 0.0956 - acc: 0.9639 - fmeasure: 0.9504 - val_loss: 0.1252 - val_acc: 0.9510 - val_fmeasure: 0.9323\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.12550 to 0.12521, saving model to mobilenet_all_layers.h5\n",
      "Epoch 7/100\n",
      "150/150 [==============================] - 51s 342ms/step - loss: 0.0851 - acc: 0.9687 - fmeasure: 0.9570 - val_loss: 0.1191 - val_acc: 0.9531 - val_fmeasure: 0.9354\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.12521 to 0.11905, saving model to mobilenet_all_layers.h5\n",
      "Epoch 8/100\n",
      "150/150 [==============================] - 51s 343ms/step - loss: 0.0774 - acc: 0.9718 - fmeasure: 0.9613 - val_loss: 0.1279 - val_acc: 0.9512 - val_fmeasure: 0.9325\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/100\n",
      "150/150 [==============================] - 51s 341ms/step - loss: 0.0692 - acc: 0.9752 - fmeasure: 0.9660 - val_loss: 0.1127 - val_acc: 0.9567 - val_fmeasure: 0.9404\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.11905 to 0.11271, saving model to mobilenet_all_layers.h5\n",
      "Epoch 10/100\n",
      "150/150 [==============================] - 51s 341ms/step - loss: 0.0638 - acc: 0.9775 - fmeasure: 0.9691 - val_loss: 0.1077 - val_acc: 0.9583 - val_fmeasure: 0.9426\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.11271 to 0.10769, saving model to mobilenet_all_layers.h5\n",
      "Epoch 11/100\n",
      "150/150 [==============================] - 51s 340ms/step - loss: 0.0574 - acc: 0.9803 - fmeasure: 0.9729 - val_loss: 0.1117 - val_acc: 0.9577 - val_fmeasure: 0.9416\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/100\n",
      "150/150 [==============================] - 52s 343ms/step - loss: 0.0526 - acc: 0.9819 - fmeasure: 0.9751 - val_loss: 0.1015 - val_acc: 0.9613 - val_fmeasure: 0.9470\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.10769 to 0.10154, saving model to mobilenet_all_layers.h5\n",
      "Epoch 13/100\n",
      "150/150 [==============================] - 51s 341ms/step - loss: 0.0489 - acc: 0.9833 - fmeasure: 0.9771 - val_loss: 0.1144 - val_acc: 0.9580 - val_fmeasure: 0.9421\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/100\n",
      "150/150 [==============================] - 51s 342ms/step - loss: 0.0435 - acc: 0.9856 - fmeasure: 0.9803 - val_loss: 0.1186 - val_acc: 0.9565 - val_fmeasure: 0.9396\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/100\n",
      "150/150 [==============================] - 51s 342ms/step - loss: 0.0410 - acc: 0.9864 - fmeasure: 0.9813 - val_loss: 0.1062 - val_acc: 0.9610 - val_fmeasure: 0.9463\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/100\n",
      "150/150 [==============================] - 52s 344ms/step - loss: 0.0376 - acc: 0.9878 - fmeasure: 0.9833 - val_loss: 0.1016 - val_acc: 0.9619 - val_fmeasure: 0.9478\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/100\n",
      "150/150 [==============================] - 51s 342ms/step - loss: 0.0347 - acc: 0.9888 - fmeasure: 0.9847 - val_loss: 0.1139 - val_acc: 0.9595 - val_fmeasure: 0.9440\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3935d98fd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "model.fit_generator(train_datagen.flow(X_tra, y_tra, batch_size=batch_size),                     \n",
    "                    steps_per_epoch=len(X_tra) / batch_size, epochs=100,\n",
    "                    validation_data=val_datagen.flow(X_val, y_val, batch_size=batch_size), \n",
    "                    validation_steps = len(X_val)/batch_size, callbacks=[early_stp, model_ckpt], workers = 10, max_queue_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 395/5400 [00:09<02:06, 39.63it/s]/home/faisal/anaconda3/envs/ai/lib/python3.6/site-packages/tqdm/_monitor.py:89: TqdmSynchronisationWarning: Set changed size during iteration (see https://github.com/tqdm/tqdm/issues/481)\n",
      "  TqdmSynchronisationWarning)\n",
      "100%|██████████| 5400/5400 [01:17<00:00, 69.85it/s]\n"
     ]
    }
   ],
   "source": [
    "test_imgs = get_imgs(test_data_dir, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen.fit(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 8s 709ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict_generator(test_datagen.flow(test_imgs, batch_size=512, shuffle=False), verbose=1, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_name</th>\n",
       "      <th>attrib_01</th>\n",
       "      <th>attrib_02</th>\n",
       "      <th>attrib_03</th>\n",
       "      <th>attrib_04</th>\n",
       "      <th>attrib_05</th>\n",
       "      <th>attrib_06</th>\n",
       "      <th>attrib_07</th>\n",
       "      <th>attrib_08</th>\n",
       "      <th>attrib_09</th>\n",
       "      <th>...</th>\n",
       "      <th>attrib_76</th>\n",
       "      <th>attrib_77</th>\n",
       "      <th>attrib_78</th>\n",
       "      <th>attrib_79</th>\n",
       "      <th>attrib_80</th>\n",
       "      <th>attrib_81</th>\n",
       "      <th>attrib_82</th>\n",
       "      <th>attrib_83</th>\n",
       "      <th>attrib_84</th>\n",
       "      <th>attrib_85</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Image-1.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Image-2.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Image-3.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Image-4.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Image-5.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Image_name  attrib_01  attrib_02  attrib_03  attrib_04  attrib_05  \\\n",
       "0  Image-1.jpg          0          0          0          0          0   \n",
       "1  Image-2.jpg          0          1          0          1          1   \n",
       "2  Image-3.jpg          0          0          0          1          0   \n",
       "3  Image-4.jpg          1          0          0          0          0   \n",
       "4  Image-5.jpg          0          0          0          1          0   \n",
       "\n",
       "   attrib_06  attrib_07  attrib_08  attrib_09    ...      attrib_76  \\\n",
       "0          1          0          0          1    ...              0   \n",
       "1          1          1          0          0    ...              0   \n",
       "2          0          1          0          0    ...              1   \n",
       "3          1          1          0          0    ...              0   \n",
       "4          1          0          0          0    ...              0   \n",
       "\n",
       "   attrib_77  attrib_78  attrib_79  attrib_80  attrib_81  attrib_82  \\\n",
       "0          0          0          1          1          0          0   \n",
       "1          1          0          1          0          0          0   \n",
       "2          0          0          1          0          0          0   \n",
       "3          1          1          1          0          0          0   \n",
       "4          0          0          1          1          0          0   \n",
       "\n",
       "   attrib_83  attrib_84  attrib_85  \n",
       "0          1          0          1  \n",
       "1          0          1          1  \n",
       "2          1          0          1  \n",
       "3          1          0          1  \n",
       "4          1          0          0  \n",
       "\n",
       "[5 rows x 86 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.read_csv('./data/sample_submission.csv')\n",
    "sub.iloc[:, 1:] = pred.round().astype(int)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5400, 86)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Thoughts\n",
    "\n",
    "This submission should get you around $\\approx$0.80 on the LB. and if you've noticed that our last epochs val_fmeasure is the same so it means that our val set is represantion of the test set and you can train for many epochs with EarlyStopping without worring about overfitting on val or train set.\n",
    "\n",
    "### How to improve from here?\n",
    "You can change many things which will let you get higher LB score. Following is a small list\n",
    "<ul>\n",
    "    <li> Change the Image size to bigger number </li>\n",
    "    <li> Increse the number of epoch in the fully trainable network($2^{nd}$ training) </li>\n",
    "    <li> Use diffrent architechure. you'll get more info on that [here](keras.io/applications/)</li>\n",
    "    <li> If nothing works ensemble is your best friend </li>\n",
    "</ul>\n",
    "\n",
    "I'll try to keep improving this notebook. Feel free to contribuite.\n",
    "\n",
    "Thanks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
